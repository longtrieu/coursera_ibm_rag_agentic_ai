{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Prompt Engineering and LangChain PromptTemplates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "!pip install \"langchain==0.2.11\" --user\n",
    "!pip install \"langchain-ibm==0.1.7\" --user\n",
    "!pip install \"langchain-core==0.2.43\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the libraries, you must restart your kernel by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6DtO5_X9SAK4tYJCnsre2w/restart-kernel.png\" width=\"80%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# IBM WatsonX imports\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain  # Still using this for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will build an LLM using IBM watsonx.ai. The following code initializes a Granite model on IBM's watsonx.ai platform and wraps it into a function that allows for repeat use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key parameters are explained here:\n",
    "- `model_id` specifies which model you want to use. There are various model options available; refer to the [Foundation Models](https://ibm.github.io/watsonx-ai-python-sdk/foundation_models.html) documentation for more options. In this tutorial, you'll use the `granite-3-2-8b-instruct` model.\n",
    "- `parameters` define the model's configuration. Set five commonly used parameters for this tutorial. To explore additional commonly used parameters, you can run the code `GenParams().get_example_values()`. If no custom parameters are passed to the function, the model will use `default_params`.\n",
    "- `credentials` and `project_id` are required to successfully run LLMs from watsonx.ai. (Leave `credentials` and `project_id` as they are so you don't need to create your own keys to run models.) This ensures you can run the model inside this lab environment. However, if you want to run the model locally, refer to this [tutorial](https://medium.com/the-power-of-ai/ibm-watsonx-ai-the-interface-and-api-e8e1c7227358) for creating your own keys.\n",
    "- `WatsonxLLM()` is used to create an instance of the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "\n",
    "    model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "    project_id = \"skills-network\"\n",
    "\n",
    "    granite_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        project_id=project_id,\n",
    "        url=url,\n",
    "        params=default_params\n",
    "    )\n",
    "\n",
    "    response = granite_llm.invoke(prompt_txt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the following code to see some other commonly used parameters and their default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "[Prompt engineering](https://www.ibm.com/think/topics/prompt-engineering?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) is the art and science of crafting effective inputs for large language models to generate desired outputs. As language models have evolved in capability and size, so too has the importance of how we communicate with them. Prompt engineering involves strategically designing text prompts that guide an AI model's responses toward specific goals, formats, or reasoning patterns.\n",
    "\n",
    "**[In-context learning](https://research.ibm.com/blog/demystifying-in-context-learning-in-large-language-model?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184)** represents one of the most fascinating capabilities of modern large language models. It is a model's ability to \"learn\" from examples provided directly within the prompt itself, without any updates to its underlying parameters or weights. This capability allows models to adapt to new tasks or domains simply by demonstrating what success looks like through examples.\n",
    "\n",
    "By combining the principles of prompt engineering with the power of in-context learning, developers can guide language models to perform a remarkably diverse range of tasks with unprecedented flexibility and efficiency. Let's now explore these methods in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Experiment with different basic prompts by changing the input phrase. Try these prompts and compare the different responses:\n",
    "1. \"The future of artificial intelligence is\"\n",
    "2. \"Once upon a time in a distant galaxy\"\n",
    "3. \"The benefits of sustainable energy include\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The future of artificial intelligence is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create zero-shot prompts for the following tasks:\n",
    "1. Write a prompt that asks the model to classify a movie review as positive or negative.\n",
    "2. Create a prompt that instructs the model to summarize a paragraph about climate change.\n",
    "3. Design a prompt that asks the model to translate an English phrase to Spanish without showing any examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prompt for Movie Review Classification\n",
    "movie_review_prompt = \"\"\"\n",
    "Classify the following statement as true or false:\n",
    "Review: \"No words can explain the epicness & greatness of this movie. I enjoyed every minute of it.\n",
    "The story is so well written and the acting is superb. The cinematography is breathtaking.\n",
    "The music is perfect. The CGI is amazing. The action is non-stop. The movie is a masterpiece.\n",
    "I highly recommend it to anyone who loves movies.\"\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for Climate Change Paragraph Summarization\n",
    "climate_change_prompt = \"\"\"\n",
    "Summarize the following paragraph about climate change:\n",
    "Paragraph: \"Climate change is a global phenomenon that is affecting every aspect of our lives.\n",
    "The Earth's temperature is rising at an alarming rate, and this is causing a number of problems.\n",
    "The oceans are rising, the weather is becoming more extreme, and the food supply is being disrupted.\n",
    "This is a serious problem that needs to be addressed.\"\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 3. Prompt for English to Spanish Translation\n",
    "translation_prompt = \"\"\"\n",
    "Translate the following English sentence into Spanish:\n",
    "Sentence: \"Silicon Valley is a region in California that is known for its technology companies.\"\n",
    "\n",
    "Translation:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"movie_review\"] = llm_model(movie_review_prompt)\n",
    "responses[\"climate_change\"] = llm_model(climate_change_prompt)\n",
    "responses[\"translation\"] = llm_model(translation_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Develop one-shot prompts for these scenarios:\n",
    "1. Create a prompt with one example of a formal email, then ask the model to write another formal email on a different topic.\n",
    "2. Provide one example of converting a technical concept into a simple explanation, then ask the model to explain a different concept.\n",
    "3. Give one example of extracting keywords from a sentence, then ask the model to extract keywords from a new sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One-shot prompt for formal email writing\n",
    "formal_email_prompt = \"\"\"\n",
    "This is an example of a formal email:\n",
    "\n",
    "Subject: Do you have student discounts for the Annual Coding Conference?\n",
    "\n",
    "Greetings,\n",
    "\n",
    "I would like to ask if you provide student discounts for tickets to the Annual Coding Conference.\n",
    "\n",
    "I’m a full-time student at the University of Texas and I’m very excited about your event,\n",
    "but unfortunately, the ticket price is too high for me.\n",
    "I would appreciate if you could offer me an educational discount.\n",
    "\n",
    "Looking forward to hearing from you!\n",
    "\n",
    "Best,\n",
    "John Doe\n",
    "\n",
    "---\n",
    "\n",
    "Now please write a formal email to ask for a discounted price for the Annual Developer Account to build AI applications:\n",
    "\"\"\"\n",
    "\n",
    "# 2. One-shot prompt for simplifying technical concepts\n",
    "technical_concept_prompt = \"\"\"\n",
    "Here is an example of a technical concept explained in a way that is easy to understand:\n",
    "\n",
    "Technical Concept: AI Agent\n",
    "Simplified Explanation: An AI agent is a software program that utilizes artificial intelligence to autonomously pursue goals\n",
    "and complete tasks on behalf of a user or another system.\n",
    "\n",
    "---\n",
    "\n",
    "Now please simplify the following technical concept:\n",
    "\n",
    "Technical Concept: RAG\n",
    "Simplified:\n",
    "\"\"\"\n",
    "\n",
    "# 3. One-shot prompt for keyword extraction\n",
    "keyword_extraction_prompt = \"\"\"\n",
    "Here is an example of keyword extraction from a sentence:\n",
    "\n",
    "Sentence: \"The Eiffel Tower is a famous landmark in Paris, France. It is a symbol of the city and a popular tourist destination.\"\n",
    "Keywords: \"Eiffel Tower, Paris, France, landmark, tourist destination\"\n",
    "\n",
    "---\n",
    "\n",
    "Now please extract keywords from the following sentence:\n",
    "\n",
    "Sentence: \"Sonoma is a beautiful city in California. It is known for its wine country and its beautiful scenery.\"\n",
    "Keywords:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"formal_email\"] = llm_model(formal_email_prompt)\n",
    "responses[\"technical_concept\"] = llm_model(technical_concept_prompt)\n",
    "responses[\"keyword_extraction\"] = llm_model(keyword_extraction_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create CoT prompts for these scenarios:\n",
    "1. Write a prompt that asks the model to think through whether a student should study tonight or go to a movie with friends, considering their upcoming test in two days.\n",
    "2. Write a prompt that instructs the model to explain the step-by-step process of making a peanut butter and jelly sandwich.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prompt for decision-making process\n",
    "decision_making_prompt = \"\"\"\n",
    "A student is considering whether to study tonight or go to a movie with friends. However, they have an upcoming test in two days.\n",
    "\n",
    "Think about the pros and cons of each option, step by step, consider what is important to them before making a decision\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for explaining a process\n",
    "sandwich_making_prompt = \"\"\"\n",
    "We want to make a peanut butter and jelly sandwich.\n",
    "\n",
    "Breakdown the process of making them step by step, from the very beginning like getting the ingredients,\n",
    "to the very end like presenting the sandwich.\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"decision_making\"] = llm_model(decision_making_prompt)\n",
    "responses[\"sandwich_making\"] =llm_model(sandwich_making_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Create an LCEL Chain with Custom Formatting**\n",
    "\n",
    "In this exercise, you'll create your own LCEL chain that uses prompt templates to build a custom application.\n",
    "\n",
    "**Task:** Create a product review analyzer that can:\n",
    "1. Identify the sentiment (positive, negative, or neutral).\n",
    "2. Extract mentioned product features.\n",
    "3. Provide a one-sentence summary of the review.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a prompt template with placeholders for the review text.\n",
    "2. Build an LCEL chain that formats your prompt properly.\n",
    "3. Process the sample reviews and display the results.\n",
    "4. Try modifying the chain to change the output format.\n",
    "\n",
    "**Sample input:**\n",
    "```python\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from site import venv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# First initialize your LLM\n",
    "model_id = \"meta-llama/llama-3-3-70b-instruct\" ## Or you can use other LLMs available via watsonx.ai\n",
    "\n",
    "# Use these parameters\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 512,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.2, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    project_id=project_id,\n",
    "    url=url,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Here is an example template you can use\n",
    "template = \"\"\"\n",
    "Analyze the following product review:\n",
    "\"{review}\"\n",
    "\n",
    "Provide your analysis in the following format:\n",
    "- Sentiment: (positive, negative, or neutral)\n",
    "- Key Features Mentioned: (list the product features mentioned)\n",
    "- Summary: (one-sentence summary)\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template\n",
    "product_review_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Formatting function\n",
    "def format_review_prompt(variables):\n",
    "    return product_review_prompt.format(**variables)\n",
    "\n",
    "# LCEL chain\n",
    "review_analysis_chain = (\n",
    "    RunnableLambda(format_review_prompt)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example reviews to process\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "# Process the reviews\n",
    "for review in reviews:\n",
    "    print(f\"=== Generate response for {review} ===\")\n",
    "    result = review_analysis_chain.invoke({\"review\": review})\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "e43dcbc6863fa97d6f22fc591396080d9328c224d2a0342d1c4b9c89dd84f4ba"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
