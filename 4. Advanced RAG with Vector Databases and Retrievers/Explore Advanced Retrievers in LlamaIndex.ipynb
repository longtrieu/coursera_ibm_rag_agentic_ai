{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef8ee8e-ed1c-4e09-b5e4-3cd98ebd3712",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c9403-9136-4390-98b4-77e44ab9a4e1",
   "metadata": {},
   "source": [
    "# **Explore Advanced Retrievers in LlamaIndex**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972315af-5e5b-4a18-a04f-33696a16a84c",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72425da0-23e3-4ef8-a1b9-c05fe2d963f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index==0.12.49 \\\n",
    "    llama-index-embeddings-huggingface==0.5.5 \\\n",
    "    llama-index-llms-ibm==0.4.0 \\\n",
    "    llama-index-retrievers-bm25==0.5.2 \\\n",
    "    sentence-transformers==5.0.0 \\\n",
    "    rank-bm25==0.2.2 \\\n",
    "    PyStemmer==2.2.0.3 \\\n",
    "    ibm-watsonx-ai==1.3.31 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac0c56-c801-4ef1-b931-c3923c16f3a6",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "We import all the necessary libraries for this lab, including core LlamaIndex components, retrievers, and IBM watsonx.ai integration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e923b35-5a45-44bb-b94f-187503f710bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Document,\n",
    "    Settings,\n",
    "    DocumentSummaryIndex,\n",
    "    KeywordTableIndex\n",
    ")\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    AutoMergingRetriever,\n",
    "    RecursiveRetriever,\n",
    "    QueryFusionRetriever\n",
    ")\n",
    "from llama_index.core.indices.document_summary import (\n",
    "    DocumentSummaryIndexLLMRetriever,\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Advanced retriever imports\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# IBM WatsonX LlamaIndex integration\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "\n",
    "# Sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Statistical libraries for fusion techniques\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"âš ï¸ scipy not available - some advanced fusion features will be limited\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049a1e2-7449-42a3-ae15-61ec6397d658",
   "metadata": {},
   "source": [
    "## watsonx.ai LLM Integration\n",
    "\n",
    "We'll create custom wrapper classes to integrate IBM watsonx.ai with LlamaIndex. This allows us to use watsonx.ai foundation models while maintaining compatibility with all LlamaIndex retrievers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed940d-7ff2-437a-8cb2-520c0f0a1fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watsonx.ai LLM using official LlamaIndex integration\n",
    "def create_watsonx_llm():\n",
    "    \"\"\"Create watsonx.ai LLM instance using official LlamaIndex integration.\"\"\"\n",
    "    try:\n",
    "        # Create the API client object\n",
    "        api_client = APIClient({'url': \"https://us-south.ml.cloud.ibm.com\"})\n",
    "        # Use llama-index-llms-ibm (official watsonx.ai integration)\n",
    "        llm = WatsonxLLM(\n",
    "            model_id=\"ibm/granite-3-3-8b-instruct\",\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "            project_id=\"skills-network\",\n",
    "            api_client=api_client,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        print(\"âœ… watsonx.ai LLM initialized using official LlamaIndex integration\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ watsonx.ai initialization error: {e}\")\n",
    "        print(\"Falling back to mock LLM for demonstration\")\n",
    "\n",
    "        # Fallback mock LLM for demonstration\n",
    "        from llama_index.core.llms.mock import MockLLM\n",
    "        return MockLLM(max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc63fe-56e0-4017-bb2e-daa10cfe3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model first\n",
    "print(\"ðŸ”§ Initializing HuggingFace embeddings...\")\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "print(\"âœ… HuggingFace embeddings initialized!\")\n",
    "\n",
    "# Setup with watsonx.ai\n",
    "print(\"ðŸ”§ Initializing watsonx.ai LLM...\")\n",
    "llm = create_watsonx_llm()\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "print(\"âœ… watsonx.ai LLM and embeddings configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd7269-4601-4d91-8604-ac04b83072d4",
   "metadata": {},
   "source": [
    "#### Sample Data Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd09a1-f2d1-4948-911b-6bc756965895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for the lab - AI/ML focused documents\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
    "    \"Reinforcement learning is a type of machine learning where agents learn to make decisions through rewards and penalties.\",\n",
    "    \"Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\",\n",
    "    \"Unsupervised learning finds hidden patterns in data without labeled examples.\",\n",
    "    \"Transfer learning leverages knowledge from pre-trained models to improve performance on new tasks.\",\n",
    "    \"Generative AI can create new content including text, images, code, and more.\",\n",
    "    \"Large language models are trained on vast amounts of text data to understand and generate human-like text.\"\n",
    "]\n",
    "\n",
    "# Consistent query examples used throughout the lab\n",
    "DEMO_QUERIES = {\n",
    "    \"basic\": \"What is machine learning?\",\n",
    "    \"technical\": \"neural networks deep learning\",\n",
    "    \"learning_types\": \"different types of learning\",\n",
    "    \"advanced\": \"How do neural networks work in deep learning?\",\n",
    "    \"applications\": \"What are the applications of AI?\",\n",
    "    \"comprehensive\": \"What are the main approaches to machine learning?\",\n",
    "    \"specific\": \"supervised learning techniques\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“„ Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\n",
    "print(f\"ðŸ” Prepared {len(DEMO_QUERIES)} consistent demo queries\")\n",
    "for i, doc in enumerate(SAMPLE_DOCUMENTS[:3], 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d4544-2561-46f5-ba18-721bd72d5f9a",
   "metadata": {},
   "source": [
    "## Initialize Lab Environment\n",
    "\n",
    "Let's create our lab class and initialize all the indexes we'll need for different retrievers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c18cf6-59b0-4557-9a7c-4ccc0916b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetrieversLab:\n",
    "    def __init__(self):\n",
    "        print(\"ðŸš€ Initializing Advanced Retrievers Lab...\")\n",
    "        self.documents = [Document(text=text) for text in SAMPLE_DOCUMENTS]\n",
    "        self.nodes = SentenceSplitter().get_nodes_from_documents(self.documents)\n",
    "\n",
    "        print(\"ðŸ“Š Creating indexes...\")\n",
    "        # Create various indexes\n",
    "        self.vector_index = VectorStoreIndex.from_documents(self.documents)\n",
    "        self.document_summary_index = DocumentSummaryIndex.from_documents(self.documents)\n",
    "        self.keyword_index = KeywordTableIndex.from_documents(self.documents)\n",
    "\n",
    "        print(\"âœ… Advanced Retrievers Lab Initialized!\")\n",
    "        print(f\"ðŸ“„ Loaded {len(self.documents)} documents\")\n",
    "        print(f\"ðŸ”¢ Created {len(self.nodes)} nodes\")\n",
    "\n",
    "# Initialize the lab\n",
    "lab = AdvancedRetrieversLab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95924dd6-60be-4c06-bce0-06095ec56334",
   "metadata": {},
   "source": [
    "#### Vector Index Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902624c-685e-4085-bb1a-91c3c1dcd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. VECTOR INDEX RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic vector retriever\n",
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=lab.vector_index,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# Alternative creation method\n",
    "alt_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query = DEMO_QUERIES[\"basic\"]  # \"What is machine learning?\"\n",
    "nodes = vector_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\")\n",
    "    print(f\"   Text: {node.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c6b00-2098-450e-9b63-f9a5f97129f0",
   "metadata": {},
   "source": [
    "#### BM25 Retriever - Advanced Keyword-Based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d36098-fe37-436b-adb3-8b1b608fa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"2. BM25 RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import Stemmer\n",
    "\n",
    "    # Create BM25 retriever with default parameters\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=lab.nodes,\n",
    "        similarity_top_k=3,\n",
    "        stemmer=Stemmer.Stemmer(\"english\"),\n",
    "        language=\"english\"\n",
    "    )\n",
    "\n",
    "    query = DEMO_QUERIES[\"technical\"]  # \"neural networks deep learning\"\n",
    "    nodes = bm25_retriever.retrieve(query)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"BM25 analyzes exact keyword matches with sophisticated scoring\")\n",
    "    print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        score = node.score if hasattr(node, 'score') and node.score else 0\n",
    "        print(f\"{i}. BM25 Score: {score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "\n",
    "        # Highlight which query terms appear in the text\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "        if found_terms:\n",
    "            print(f\"   â†’ Found terms: {found_terms}\")\n",
    "        print()\n",
    "\n",
    "    print(\"BM25 vs TF-IDF Comparison:\")\n",
    "    print(\"TF-IDF Problem: Linear term frequency scaling\")\n",
    "    print(\"  Example: 10 occurrences â†’ score of 10, 100 occurrences â†’ score of 100\")\n",
    "    print(\"BM25 Solution: Saturation function\")\n",
    "    print(\"  Example: 10 occurrences â†’ high score, 100 occurrences â†’ slightly higher score\")\n",
    "    print()\n",
    "    print(\"TF-IDF Problem: No document length consideration\")\n",
    "    print(\"  Example: Long documents dominate results\")\n",
    "    print(\"BM25 Solution: Length normalization (b parameter)\")\n",
    "    print(\"  Example: Scores adjusted based on document length vs. average\")\n",
    "    print()\n",
    "    print(\"Key BM25 Parameters:\")\n",
    "    print(\"- k1 â‰ˆ 1.2: Term frequency saturation (how quickly scores plateau)\")\n",
    "    print(\"- b â‰ˆ 0.75: Document length normalization (0=none, 1=full)\")\n",
    "    print(\"- IDF weighting: Rare terms get higher scores\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ BM25Retriever requires 'pip install PyStemmer'\")\n",
    "    print(\"Demonstrating BM25 concepts with fallback vector search...\")\n",
    "\n",
    "    fallback_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "    query = DEMO_QUERIES[\"technical\"]\n",
    "    nodes = fallback_retriever.retrieve(query)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"(Using vector fallback to demonstrate BM25 concepts)\")\n",
    "\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Vector Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "\n",
    "        # Demonstrate TF-IDF concept manually\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "\n",
    "        if found_terms:\n",
    "            print(f\"   â†’ BM25 would boost this result for terms: {found_terms}\")\n",
    "        print()\n",
    "\n",
    "    print(\"BM25 Concept Demonstration:\")\n",
    "    print(\"1. TF-IDF Foundation:\")\n",
    "    print(\"   - Term Frequency: How often words appear in document\")\n",
    "    print(\"   - Inverse Document Frequency: How rare words are across collection\")\n",
    "    print(\"   - TF-IDF = TF Ã— IDF (balances frequency vs rarity)\")\n",
    "    print()\n",
    "    print(\"2. BM25 Improvements:\")\n",
    "    print(\"   - Saturation: Prevents over-scoring repeated terms\")\n",
    "    print(\"   - Length normalization: Prevents long document bias\")\n",
    "    print(\"   - Tunable parameters: k1 (saturation) and b (length adjustment)\")\n",
    "    print()\n",
    "    print(\"3. Real-world Usage:\")\n",
    "    print(\"   - Elasticsearch default scoring function\")\n",
    "    print(\"   - Apache Lucene/Solr standard\")\n",
    "    print(\"   - Used in 83% of text-based recommender systems\")\n",
    "    print(\"   - Developed by Robertson & SpÃ¤rck Jones at City University London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64251976-c241-4224-9ae9-eaa1f24e7655",
   "metadata": {},
   "source": [
    "#### Document Summary Index Retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314066bf-3c31-4188-a747-ed6607fb1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3. DOCUMENT SUMMARY INDEX RETRIEVERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LLM-based document summary retriever\n",
    "doc_summary_retriever_llm = DocumentSummaryIndexLLMRetriever(\n",
    "    lab.document_summary_index,\n",
    "    choice_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "# Embedding-based document summary retriever\n",
    "doc_summary_retriever_embedding = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    lab.document_summary_index,\n",
    "    similarity_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"learning_types\"]  # \"different types of learning\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "print(\"\\nA) LLM-based Document Summary Retriever:\")\n",
    "print(\"Uses LLM to select relevant documents based on summaries\")\n",
    "try:\n",
    "    nodes_llm = doc_summary_retriever_llm.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_llm)} nodes\")\n",
    "    for i, node in enumerate(nodes_llm[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"LLM-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"B) Embedding-based Document Summary Retriever:\")\n",
    "print(\"Uses vector similarity between query and document summaries\")\n",
    "try:\n",
    "    nodes_emb = doc_summary_retriever_embedding.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_emb)} nodes\")\n",
    "    for i, node in enumerate(nodes_emb[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Embedding-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"Document Summary Index workflow:\")\n",
    "print(\"1. Generates summaries for each document using LLM\")\n",
    "print(\"2. Uses summaries to select relevant documents\")\n",
    "print(\"3. Returns full content from selected documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff964748-739a-4d8d-8e7d-9a8fd3ab9beb",
   "metadata": {},
   "source": [
    "#### Auto Merging Retriever - Hierarchical Context Preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718cd9c-db90-4816-a872-70e48082a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4. AUTO MERGING RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create hierarchical nodes\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[512, 256, 128]\n",
    ")\n",
    "\n",
    "hier_nodes = node_parser.get_nodes_from_documents(lab.documents)\n",
    "\n",
    "# Create storage context with all nodes\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(hier_nodes)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Create base index\n",
    "base_index = VectorStoreIndex(hier_nodes, storage_context=storage_context)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# Create auto-merging retriever\n",
    "auto_merging_retriever = AutoMergingRetriever(\n",
    "    base_retriever,\n",
    "    storage_context,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"advanced\"]  # \"How do neural networks work in deep learning?\"\n",
    "nodes = auto_merging_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Auto-merged to {len(nodes)} nodes\")\n",
    "for i, node in enumerate(nodes[:3], 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Auto-merged)\")\n",
    "    print(f\"   Text: {node.text[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbc028-6375-4586-a63a-621bf64b8d3a",
   "metadata": {},
   "source": [
    "#### Recursive Retriever - Multi-Level Reference Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400333fc-020d-4b8b-a869-62280a249a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"5. RECURSIVE RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create documents with references\n",
    "docs_with_refs = []\n",
    "for i, doc in enumerate(lab.documents):\n",
    "    # Add reference metadata\n",
    "    ref_doc = Document(\n",
    "        text=doc.text,\n",
    "        metadata={\n",
    "            \"doc_id\": f\"doc_{i}\",\n",
    "            \"references\": [f\"doc_{j}\" for j in range(len(lab.documents)) if j != i][:2]\n",
    "        }\n",
    "    )\n",
    "    docs_with_refs.append(ref_doc)\n",
    "\n",
    "# Create index with referenced documents\n",
    "ref_index = VectorStoreIndex.from_documents(docs_with_refs)\n",
    "\n",
    "# Create retriever mapping\n",
    "retriever_dict = {\n",
    "    f\"doc_{i}\": ref_index.as_retriever(similarity_top_k=1)\n",
    "    for i in range(len(docs_with_refs))\n",
    "}\n",
    "\n",
    "# Base retriever\n",
    "base_retriever = ref_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# Add the root retriever to the dictionary\n",
    "retriever_dict[\"vector\"] = base_retriever\n",
    "\n",
    "# Recursive retriever\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict=retriever_dict,\n",
    "    query_engine_dict={},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"applications\"]  # \"What are the applications of AI?\"\n",
    "try:\n",
    "    nodes = recursive_retriever.retrieve(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Recursively retrieved {len(nodes)} nodes\")\n",
    "    for i, node in enumerate(nodes[:3], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Recursive)\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Recursive retriever demo: {str(e)}\")\n",
    "    print(\"Note: Recursive retriever requires specific node reference setup\")\n",
    "\n",
    "    # Fallback to basic retrieval for demonstration\n",
    "    print(\"\\nFalling back to basic retrieval demonstration...\")\n",
    "    base_nodes = base_retriever.retrieve(query)\n",
    "    for i, node in enumerate(base_nodes[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180896e-3112-477f-8f47-5a6f52ab17f8",
   "metadata": {},
   "source": [
    "#### Query Fusion Retriever - Multi-Query Enhancement with Advanced Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3694372-d230-4b5f-84fd-5267247f03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6. QUERY FUSION RETRIEVER - OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create base retriever\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"QueryFusionRetriever generates multiple query variations and fuses results\")\n",
    "print(\"using one of three sophisticated fusion modes.\")\n",
    "\n",
    "print(\"\\nOverview of Fusion Modes:\")\n",
    "print(\"1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\")\n",
    "print(\"2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\")\n",
    "print(\"3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\")\n",
    "\n",
    "print(\"\\nDemonstration workflow:\")\n",
    "print(\"Each subsection below explores one fusion mode in detail with:\")\n",
    "print(\"- Theoretical explanation of the fusion method\")\n",
    "print(\"- Live demonstration using QueryFusionRetriever\")\n",
    "print(\"- Manual implementation showing the underlying mathematics\")\n",
    "print(\"- Use case recommendations and trade-offs\")\n",
    "\n",
    "print(f\"\\nUsing consistent test query throughout: '{query}'\")\n",
    "print(\"This allows direct comparison of how each fusion mode handles the same input.\")\n",
    "\n",
    "print(\"\\nProceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824247da-ed08-4e48-9f82-91259abbb9d9",
   "metadata": {},
   "source": [
    "##### Reciprocal Rank Fusion (RRF) Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74e0c6-ff56-480d-9a4a-ff3c8e630a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create QueryFusionRetriever with RRF mode\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with reciprocal_rerank mode:\")\n",
    "print(\"This demonstrates how RRF works within the query fusion framework\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with RRF mode\n",
    "    rrf_query_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"reciprocal_rerank\",\n",
    "        use_async=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever will:\")\n",
    "    print(\"1. Generate query variations using LLM\")\n",
    "    print(\"2. Retrieve results for each variation\")\n",
    "    print(\"3. Apply Reciprocal Rank Fusion\")\n",
    "\n",
    "    nodes = rrf_query_fusion.retrieve(query)\n",
    "\n",
    "    print(f\"\\nRRF Query Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Final RRF Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"RRF Benefits in Query Fusion Context:\")\n",
    "    print(\"- Automatically handles query variations of different quality\")\n",
    "    print(\"- No bias toward queries that return higher raw scores\")\n",
    "    print(\"- Stable performance across diverse query formulations\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating RRF concept manually with query variations...\")\n",
    "\n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "\n",
    "    print(\"Manual RRF with Query Variations:\")\n",
    "    all_results = {}\n",
    "\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "\n",
    "        # Apply RRF scoring\n",
    "        for rank, node in enumerate(nodes):\n",
    "            node_id = node.node.node_id\n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'rrf_score': 0,\n",
    "                    'query_ranks': []\n",
    "                }\n",
    "\n",
    "            # Calculate RRF contribution: 1 / (rank + k)\n",
    "            k = 60  # Standard RRF parameter\n",
    "            rrf_contribution = 1.0 / (rank + 1 + k)\n",
    "            all_results[node_id]['rrf_score'] += rrf_contribution\n",
    "            all_results[node_id]['query_ranks'].append((i, rank + 1))\n",
    "\n",
    "    # Sort by final RRF score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['rrf_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCombined RRF Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Final RRF Score: {result['rrf_score']:.4f}\")\n",
    "        print(f\"   Query ranks: {result['query_ranks']}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"RRF Formula Demonstration:\")\n",
    "    print(\"For each document: RRF_score = Î£(1 / (rank + 60))\")\n",
    "    print(\"- Rank 1 in query: 1/(1+60) = 0.0164\")\n",
    "    print(\"- Rank 2 in query: 1/(2+60) = 0.0161\")\n",
    "    print(\"- Rank 3 in query: 1/(3+60) = 0.0159\")\n",
    "    print(\"Documents appearing in multiple queries get higher combined scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763e01d-ed5e-47b5-bc7c-0fb35d03847b",
   "metadata": {},
   "source": [
    "##### Relative Score Fusion Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc07d9-b2d1-46f2-9f16-b5811517bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with relative_score mode:\")\n",
    "print(\"This mode preserves score magnitudes while normalizing across query variations\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with relative score mode\n",
    "    rel_score_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"relative_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with relative_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Normalize scores within each variation (score/max_score)\")\n",
    "    print(\"3. Combine normalized scores\")\n",
    "\n",
    "    nodes = rel_score_fusion.retrieve(query)\n",
    "\n",
    "    print(f\"\\nRelative Score Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Combined Relative Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"Relative Score Benefits in Query Fusion:\")\n",
    "    print(\"- Preserves confidence information from embedding model\")\n",
    "    print(\"- Ensures fair contribution from each query variation\")\n",
    "    print(\"- More interpretable than rank-only methods\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Relative Score concept manually...\")\n",
    "\n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "\n",
    "    print(\"Manual Relative Score Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    query_max_scores = []\n",
    "\n",
    "    # Step 1: Get results and find max scores for each query\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "        max_score = max(scores) if scores else 1.0\n",
    "        query_max_scores.append(max_score)\n",
    "\n",
    "        print(f\"Max score for this query: {max_score:.4f}\")\n",
    "\n",
    "        # Store results with normalization info\n",
    "        for node in nodes:\n",
    "            node_id = node.node.node_id\n",
    "            original_score = node.score or 0\n",
    "            normalized_score = original_score / max_score if max_score > 0 else 0\n",
    "\n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "\n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': original_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "\n",
    "    # Step 2: Sort by combined relative score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCombined Relative Score Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Score breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} â†’ {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"Relative Score Normalization Process:\")\n",
    "    print(\"1. For each query variation, find max_score\")\n",
    "    print(\"2. Normalize: normalized_score = original_score / max_score\")\n",
    "    print(\"3. Sum normalized scores across all query variations\")\n",
    "    print(\"4. Documents with consistently high scores across queries win\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4737db-11bc-4024-9674-831c706c1adc",
   "metadata": {},
   "source": [
    "##### Distribution-Based Score Fusion Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a738aaa-d3e2-48e6-9ed5-e0953a1b488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=8)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with dist_based_score mode:\")\n",
    "print(\"This mode uses statistical analysis for the most sophisticated score fusion\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with distribution-based mode\n",
    "    dist_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"dist_based_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with dist_based_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Analyze score distributions for each variation\")\n",
    "    print(\"3. Apply statistical normalization (z-score, percentiles)\")\n",
    "    print(\"4. Combine with distribution-aware weighting\")\n",
    "\n",
    "    nodes = dist_fusion.retrieve(query)\n",
    "\n",
    "    print(f\"\\nDistribution-Based Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Statistically Normalized Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"Distribution-Based Benefits in Query Fusion:\")\n",
    "    print(\"- Accounts for score distribution differences between query variations\")\n",
    "    print(\"- Statistically robust against outliers and noise\")\n",
    "    print(\"- Adapts weighting based on query variation reliability\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Distribution-Based concept manually...\")\n",
    "\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        print(\"âš ï¸ Full statistical analysis requires scipy\")\n",
    "\n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "\n",
    "    print(\"Manual Distribution-Based Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    variation_stats = []\n",
    "\n",
    "    # Step 1: Collect results and analyze distributions\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "\n",
    "        # Calculate distribution statistics\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        std_score = np.std(scores) if len(scores) > 1 else 1\n",
    "        min_score = np.min(scores) if scores else 0\n",
    "        max_score = np.max(scores) if scores else 1\n",
    "\n",
    "        stats_info = {\n",
    "            'mean': mean_score,\n",
    "            'std': std_score,\n",
    "            'min': min_score,\n",
    "            'max': max_score,\n",
    "            'nodes': nodes,\n",
    "            'scores': scores\n",
    "        }\n",
    "        variation_stats.append(stats_info)\n",
    "\n",
    "        print(f\"Distribution stats: mean={mean_score:.3f}, std={std_score:.3f}\")\n",
    "        print(f\"Score range: [{min_score:.3f}, {max_score:.3f}]\")\n",
    "\n",
    "        # Apply z-score normalization\n",
    "        for node, score in zip(nodes, scores):\n",
    "            node_id = node.node.node_id\n",
    "\n",
    "            # Z-score normalization\n",
    "            if std_score > 0:\n",
    "                z_score = (score - mean_score) / std_score\n",
    "            else:\n",
    "                z_score = 0\n",
    "\n",
    "            # Convert to [0,1] using sigmoid\n",
    "            normalized_score = 1 / (1 + np.exp(-z_score))\n",
    "\n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "\n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': score,\n",
    "                'z_score': z_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "\n",
    "    # Step 2: Sort by combined distribution-based score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCombined Distribution-Based Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Statistical breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} â†’ \"\n",
    "                  f\"z={contrib['z_score']:.2f} â†’ {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"Distribution-Based Process:\")\n",
    "    print(\"1. Calculate mean and std for each query variation\")\n",
    "    print(\"2. Z-score normalize: z = (score - mean) / std\")\n",
    "    print(\"3. Sigmoid transform: normalized = 1 / (1 + exp(-z))\")\n",
    "    print(\"4. Sum normalized scores across variations\")\n",
    "    print(\"5. Results reflect statistical significance across all query forms\")\n",
    "\n",
    "# Show fusion mode comparison summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FUSION MODES COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"All three modes tested with the same query for direct comparison:\")\n",
    "print(f\"Query: {query}\")\n",
    "print()\n",
    "print(\"Mode Characteristics:\")\n",
    "print(\"â€¢ RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\")\n",
    "print(\"â€¢ Relative Score: Preserves confidence, normalizes by max score\")\n",
    "print(\"â€¢ Distribution-Based: Most sophisticated, statistical normalization\")\n",
    "print()\n",
    "print(\"Choose based on your use case:\")\n",
    "print(\"- Production stability â†’ RRF\")\n",
    "print(\"- Score interpretability â†’ Relative Score\")\n",
    "print(\"- Statistical robustness â†’ Distribution-Based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed15934-104d-4973-a835-663f18d4a4ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Now that you've learned about advanced retrievers, let's practice implementing them in different scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab764cb-5360-4032-a612-adc7a2f91353",
   "metadata": {},
   "source": [
    "## Exercise 1 - Build a Custom Hybrid Retriever\n",
    "\n",
    "Your task is to create a hybrid retriever that combines both vector similarity and BM25 keyword search for improved results.\n",
    "\n",
    "**Requirements:**\n",
    "- Use both Vector Index Retriever and BM25 Retriever\n",
    "- Implement a simple score fusion mechanism which takes a weighted average of normalized scores\n",
    "- Test with different query types (semantic vs keyword-focused)\n",
    "\n",
    "**Important Note**: Node IDs from different retrievers won't match even for the same content, so we need to match by text content instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097e950-6f54-4f93-8f54-dab89ac47cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create both retrievers\n",
    "vector_retriever = lab.vector_index.as_retriever(similarity_top_k=10)\n",
    "try:\n",
    "  bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=lab.nodes, similarity_top_k=10\n",
    "  )\n",
    "except:\n",
    "  # Fallback if BM25 is not available\n",
    "  bm25_retriever = vector_retriever\n",
    "\n",
    "# Step 2: Implement score fusion\n",
    "def hybrid_retrieve(query, top_k=5):\n",
    "  # Get results from both retrievers\n",
    "  vector_results = vector_retriever.retrieve(query)\n",
    "  bm25_results = bm25_retriever.retrieve(query)\n",
    "\n",
    "  # Create dictionaries using text content as keys (since node IDs differ)\n",
    "  vector_scores = {}\n",
    "  bm25_scores = {}\n",
    "  all_nodes = {}\n",
    "\n",
    "  # Normalize vector scores\n",
    "  max_vector_score = max([r.score for r in vector_results]) if vector_results else 1\n",
    "  for result in vector_results:\n",
    "    text_key = result.text.strip()  # Use text content as key\n",
    "    normalized_score = result.score / max_vector_score\n",
    "    vector_scores[text_key] = normalized_score\n",
    "    all_nodes[text_key] = result\n",
    "\n",
    "  # Normalize BM25 scores\n",
    "  max_bm25_score = max([r.score for r in bm25_results]) if bm25_results else 1\n",
    "  for result in bm25_results:\n",
    "    text_key = result.text.strip()  # Use text content as key\n",
    "    normalized_score = result.score / max_bm25_score\n",
    "    bm25_scores[text_key] = normalized_score\n",
    "    all_nodes[text_key] = result\n",
    "\n",
    "  # Calculate hybrid scores\n",
    "  hybrid_results = []\n",
    "  for text_key in all_nodes:\n",
    "    vector_score = vector_scores.get(text_key, 0)\n",
    "    bm25_score = bm25_scores.get(text_key, 0)\n",
    "    hybrid_score = 0.7 * vector_score + 0.3 * bm25_score\n",
    "\n",
    "    hybrid_results.append({\n",
    "      'node': all_nodes[text_key],\n",
    "      'vector_score': vector_score,\n",
    "      'bm25_score': bm25_score,\n",
    "      'hybrid_score': hybrid_score\n",
    "    })\n",
    "\n",
    "  # Sort by hybrid score and return top k\n",
    "  hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "  return hybrid_results[:top_k]\n",
    "\n",
    "# Step 3: Test with different queries\n",
    "test_queries = [\n",
    "  \"What is machine learning?\",  # Semantic query\n",
    "  \"neural networks deep learning\",  # Keyword query\n",
    "  \"supervised learning techniques\"  # Mixed query\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "  print(f\"Query: {query}\")\n",
    "  results = hybrid_retrieve(query, top_k=3)\n",
    "  for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "    print(f\"   Vector: {result['vector_score']:.3f}, BM25: {result['bm25_score']:.3f}\")\n",
    "    print(f\"   Text: {result['node'].text[:80]}...\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5644-6ec0-4c63-96e6-434de5a8388f",
   "metadata": {},
   "source": [
    "## Exercise 2 - Create a Production RAG Pipeline\n",
    "\n",
    "Build a complete RAG pipeline that uses multiple retrieval strategies and includes evaluation metrics.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement retrieval with multiple strategies\n",
    "- Add query routing logic\n",
    "- Include basic evaluation metrics that evaluate whether the pipeline succeeded or failed\n",
    "- Handle edge cases and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47468d4-89e2-493d-aa91-a5e5717ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRAGPipeline:\n",
    "  def __init__(self, index, llm):\n",
    "    self.index = index\n",
    "    self.llm = llm\n",
    "    self.vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "  def _route_query(self, question):\n",
    "    \"\"\"Simple query routing based on question characteristics\"\"\"\n",
    "    if any(word in question.lower() for word in [\"what\", \"explain\", \"describe\"]):\n",
    "      return \"semantic\"\n",
    "    elif any(word in question.lower() for word in [\"list\", \"types\", \"examples\"]):\n",
    "      return \"comprehensive\"\n",
    "    else:\n",
    "      return \"semantic\"\n",
    "\n",
    "  def query(self, question, strategy=\"auto\"):\n",
    "    try:\n",
    "      # Route query if strategy is auto\n",
    "      if strategy == \"auto\":\n",
    "        strategy = self._route_query(question)\n",
    "\n",
    "      # Retrieve relevant documents\n",
    "      if strategy == \"semantic\":\n",
    "        retriever = self.vector_retriever\n",
    "        top_k = 3\n",
    "      elif strategy == \"comprehensive\":\n",
    "        retriever = self.vector_retriever\n",
    "        top_k = 5\n",
    "      else:\n",
    "        retriever = self.vector_retriever\n",
    "        top_k = 3\n",
    "\n",
    "      # Get relevant documents\n",
    "      relevant_docs = retriever.retrieve(question)\n",
    "\n",
    "      # Prepare context\n",
    "      context = \"\\n\\n\".join([doc.text for doc in relevant_docs[:top_k]])\n",
    "\n",
    "      # Generate response\n",
    "      prompt = f\"\"\"Based on the following context, please answer the question:\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "\n",
    "      try:\n",
    "        response = self.llm.complete(prompt)\n",
    "        return {\n",
    "          \"answer\": response.text,\n",
    "          \"strategy\": strategy,\n",
    "          \"num_docs\": len(relevant_docs),\n",
    "          \"status\": \"success\"\n",
    "        }\n",
    "      except Exception as e:\n",
    "        return {\n",
    "          \"answer\": f\"Based on the retrieved documents: {context[:200]}...\",\n",
    "          \"strategy\": strategy,\n",
    "          \"num_docs\": len(relevant_docs),\n",
    "          \"status\": f\"llm_error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "      return {\n",
    "        \"answer\": \"I encountered an error processing your question.\",\n",
    "        \"strategy\": strategy,\n",
    "        \"num_docs\": 0,\n",
    "        \"status\": f\"error: {str(e)}\"\n",
    "      }\n",
    "\n",
    "  def evaluate(self, test_queries):\n",
    "    results = []\n",
    "    for query in test_queries:\n",
    "      result = self.query(query)\n",
    "      results.append({\n",
    "        \"query\": query,\n",
    "        \"result\": result,\n",
    "        \"success\": result[\"status\"] == \"success\"\n",
    "      })\n",
    "\n",
    "    success_rate = sum(1 for r in results if r[\"success\"]) / len(results)\n",
    "    return {\n",
    "      \"success_rate\": success_rate,\n",
    "      \"results\": results\n",
    "    }\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = ProductionRAGPipeline(lab.vector_index, llm)\n",
    "\n",
    "test_queries = [\n",
    "  \"What is machine learning?\",\n",
    "  \"List different types of learning algorithms\",\n",
    "  \"Explain neural networks\"\n",
    "]\n",
    "\n",
    "print(\"Testing Production RAG Pipeline:\")\n",
    "for query in test_queries:\n",
    "  result = pipeline.query(query)\n",
    "  print(f\"\\nQuery: {query}\")\n",
    "  print(f\"Strategy: {result['strategy']}\")\n",
    "  print(f\"Status: {result['status']}\")\n",
    "  print(f\"Answer: {result['answer'][:100]}...\")\n",
    "\n",
    "# Evaluate performance\n",
    "evaluation = pipeline.evaluate(test_queries)\n",
    "print(f\"\\nPipeline Success Rate: {evaluation['success_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c3ce9-cb99-463e-a4bd-bc1f8e29da60",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "38fbe24a85cef87de7f7211df2f38f951e6904d22e34f03df005ed3dfb74baf6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
