{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a207b86-f698-46d0-ab42-7474774555cb",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9f21c-65a8-4fd3-96ea-082ef92b7f9a",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Build an Image Captioning System with IBM watsonx and Granite](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26701d-f5df-48d6-91e8-b08ecb25abb2",
   "metadata": {},
   "source": [
    "## <a id='Setup'></a>[Setup](#toc0_)\n",
    "\n",
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://pypi.org/project/ibm-watsonx-ai/): `ibm-watsonx-ai` is a library that allows to work with watsonx.ai service on IBM Cloud and IBM Cloud for Data. Train, test and deploy your models as APIs for application development, share with colleagues using this python library.\n",
    "\n",
    "* `image`: `image` from Pillow is the Python Imaging Library (PIL) fork that provides easy-to-use methods for opening, manipulating, and saving image files in various formats. It’s commonly used for preprocessing images before feeding them into machine learning models or APIs.\n",
    "\n",
    "* `requests`: `requests` is a simple and intuitive HTTP library for Python. It sends all kinds of HTTP/1.1 requests with methods like GET and POST. In this lab, it downloads images from the web for analysis by the multimodal AI model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab100d-02ca-4d85-a1a3-018ac534ec58",
   "metadata": {},
   "source": [
    "### [Installing required libraries](#toc0_)\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. Please wait until it completes.\n",
    "\n",
    "This step could take **several minutes**; please be patient.\n",
    "\n",
    "**NOTE**: If you encounter any issues, please restart the kernel and run the cell again.  You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/crvBKBOkg9aBzXZiwGEXbw/Restarting-the-Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087f390-b713-4486-9fce-4517a7242a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install ibm-watsonx-ai==1.1.20 image==1.5.33 requests==2.32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fb729-d4b8-4d44-b7ea-6ae54cb4f041",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[watsonx API credentials and project_id](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092c776-dbf3-4b97-b38f-38bf543d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "import os\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    )\n",
    "\n",
    "project_id=\"skills-network\"\n",
    "client = APIClient(credentials)\n",
    "# GET TextModels ENUM\n",
    "client.foundation_models.TextModels\n",
    "\n",
    "# PRINT dict of Enums\n",
    "client.foundation_models.TextModels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eeb270-63af-4bc7-a654-2569d68c1900",
   "metadata": {},
   "source": [
    "## <a href=\"#Image-preparation\">Image preparation</a>\n",
    "\n",
    "- Download the image\n",
    "- Display the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0016db2-ac9c-4a5b-876a-ff4e98bdb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_image_1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png'\n",
    "url_image_2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png'\n",
    "url_image_3 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png'\n",
    "url_image_4 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png'\n",
    "\n",
    "image_urls = [url_image_1, url_image_2, url_image_3, url_image_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcede01b-f7e2-4cc6-8abf-030e56e4c71c",
   "metadata": {},
   "source": [
    "To gain a better understanding of our data input, let's display the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46e828-782b-42f1-81ef-89ee3cf47ba1",
   "metadata": {},
   "source": [
    "![Image 1](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png)<figcaption>Image 1</figcaption>\n",
    "\n",
    "![Image 2](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png)<figcaption>Image 2</figcaption>\n",
    "\n",
    "![Image 3](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png)<figcaption>Image 3</figcaption>\n",
    "\n",
    "![Image 4](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png)<figcaption>Image 4</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d226c33-2f83-42c0-86ac-01b7e2a69a62",
   "metadata": {},
   "source": [
    "## <a href=\"#Work-with-large-language-models-on-watsonx.ai\"></a>[Work with large language models on watsonx.ai](#toc0_)\n",
    "\n",
    "Specify the `model_id` of the model that you will use for the chat with image modalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a6f84-0dd5-4260-a4f3-aada7cc5bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'ibm/granite-vision-3-2-2b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d8a4e-cb41-4518-a7c4-3db144bdac41",
   "metadata": {},
   "source": [
    "### <a id='toc1_8_1_'></a>[Check the model parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4accb5a-e704-4fac-8049-8fbd36c404c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextChatParameters\n",
    "\n",
    "TextChatParameters.show()\n",
    "\n",
    "params = TextChatParameters(\n",
    "    temperature=0.2,\n",
    "    top_p=0.5,\n",
    "\n",
    ")\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94ce73-905d-4173-8836-5753e63c9686",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Initialize the model](#toc0_)\n",
    "\n",
    "\n",
    "Initialize the `ModelInference` class with the previously specified parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009495a5-2bd6-4fec-b7d0-a5d80428abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4705b4e-6152-4bf4-b35b-a7f12dca654c",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Encode the image](#toc0_)\n",
    "\n",
    "Encode the image to `base64.b64encode`. Why do you need to encode the image to `base64.b64encode`? JSON is a text-based format and does not support binary data. By encoding the image as a Base64 string, you can embed the image data directly within the JSON structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af328c-73ed-4d76-892f-8e1c802caa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_images_to_base64(image_urls):\n",
    "    \"\"\"\n",
    "    Downloads and encodes a list of image URLs to base64 strings.\n",
    "\n",
    "    Parameters:\n",
    "    - image_urls (list): A list of image URLs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of base64-encoded image strings.\n",
    "    \"\"\"\n",
    "    encoded_images = []\n",
    "    for url in image_urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            encoded_image = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "            encoded_images.append(encoded_image)\n",
    "            print(type(encoded_image))\n",
    "        else:\n",
    "            print(f\"Warning: Failed to fetch image from {url} (Status code: {response.status_code})\")\n",
    "            encoded_images.append(None)\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df691d66-6a23-4d95-9989-a5d264dc5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = encode_images_to_base64(image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62bfc2-b1d0-48ae-b834-c05463a3b244",
   "metadata": {},
   "source": [
    "## <a id='#Multimodal-inference-function'></a>[Multimodal inference function](#toc0_)\n",
    "\n",
    "Next, define a function to generate responses from the model.\n",
    "\n",
    "The `generate_model_response` function is designed to interact with a multimodal AI model that accepts both text and image inputs. This function takes an image, along with a user’s query, and generates a response from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b08d42-9546-492a-9078-96d60b56c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(encoded_image, user_query, assistant_prompt=\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences: \"):\n",
    "    \"\"\"\n",
    "    Sends an image and a query to the model and retrieves the description or answer.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_image (str): Base64-encoded image string.\n",
    "    - user_query (str): The user's question about the image.\n",
    "    - assistant_prompt (str): Optional prompt to guide the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's response for the given image and query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the messages object\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_prompt + user_query\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,\" + encoded_image,\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Send the request to the model\n",
    "    response = model.chat(messages=messages)\n",
    "\n",
    "    # Return the model's response\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e37b10-4d8d-4348-8222-f68ff1d9a2c4",
   "metadata": {},
   "source": [
    "## <a id='#Exercises'></a>[Exercises](#toc0_)\n",
    "\n",
    "Now, let's practice by exploring some other capabilities of this model. Try asking \"How much cholesterol is in this product?\" in the 4th image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb3e2d-4ba1-4195-8688-cd9e2129f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = encoded_images[3]\n",
    "user_query = \"How much cholesterol is in this product?\"\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2ed02-12c6-4eb5-ac32-f74c5ee63b0b",
   "metadata": {},
   "source": [
    "Try asking \"What is the color of the woman's jacket?\" in the 2nd image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88d7c5-22cc-4711-bd2a-b7aa26f62881",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = encoded_images[1]\n",
    "user_query = \"What is the color of the woman's jacket?\"\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa2389-9b92-49bc-b054-fa52058adfa4",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "b631c1541313ee4291baea188fd6b6d2f44bf4a8312116e4621f0bec039e5317"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
