{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceb17a0-f723-4ac7-b8ed-b22d917eda1b",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f85bc1-aaa7-4403-bdb1-91d841e1164e",
   "metadata": {},
   "source": [
    "## **DataWizard: AI-Powered Data Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa067a2c-1041-46e0-b050-a43d24d909e9",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18c247-e80a-4f46-aa77-5d4416d76012",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c934e-1609-400b-8a45-569756b8f3ed",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "* [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building modular AI applications with tools and agents.\n",
    "* [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting LangChain with OpenAI's language models.\n",
    "* [`openai`](https://github.com/openai/openai-python) for accessing AI models that power your conversational interface.\n",
    "* [`pandas`](https://pandas.pydata.org/) for data manipulation and analysis of CSV datasets.\n",
    "* [`numpy`](https://numpy.org/) for numerical operations and array processing.\n",
    "* [`scikit-learn`](https://scikit-learn.org/stable/) for implementing machine learning models and evaluation metrics.\n",
    "* [`matplotlib`](https://matplotlib.org/) for creating data visualizations based on analysis results.\n",
    "* [`seaborn`](https://seaborn.pydata.org/) for enhanced statistical visualizations of dataset patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4e3df-6fda-4215-96f4-8bf04a233244",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. This step could take **several minutes**; please be patient.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/crvBKBOkg9aBzXZiwGEXbw/Restarting-the-Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n",
    "\n",
    "**NOTE**: If you encounter any issues, restart the kernel and run it again by clicking the **Restart the kernel** icon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ae979-e13f-4eaa-8ba7-0414d5b95f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai==0.3.10 | tail -n 1\n",
    "%pip install langchain==0.3.21 | tail -n 1\n",
    "%pip install openai==1.68.2 | tail -n 1\n",
    "%pip install pandas==2.2.3 | tail -n 1\n",
    "%pip install numpy==2.2.4 | tail -n 1\n",
    "%pip install matplotlib==3.10.1 | tail -n 1\n",
    "%pip install seaborn==0.13.2 | tail -n 1\n",
    "%pip install scikit-learn==1.6.1 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe5277-293a-4d3d-a1c4-0a17f44bfa6c",
   "metadata": {},
   "source": [
    "Let's download the `.csv` files required in the project later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1dce2-32e4-4294-95ba-32f86fc2ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/N0CceRlquaf9q85PK759WQ/regression-dataset.csv\n",
    "! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7J73m6Nsz-vmojwab91gMA/classification-dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203abf72-195b-46de-96a4-dabb25fe62fb",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "Import all required libraries here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d60f0-1647-4e6c-9911-477e87918cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import sklearn\n",
    "import langchain\n",
    "import openai\n",
    "import langchain_openai\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be4a83-8464-493f-9831-5468eae43cd0",
   "metadata": {},
   "source": [
    "## Langchain tools\n",
    "\n",
    "### List CSV files tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f9e4-0365-4a50-8632-1abadc8fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_csv_files() -> Optional[List[str]]:\n",
    "    \"\"\"List all CSV file names in the local directory.\n",
    "\n",
    "    Returns:\n",
    "        A list containing CSV file names.\n",
    "        If no CSV files are found, returns None.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(os.getcwd(), \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    return [os.path.basename(file) for file in csv_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c649a-ab9d-4537-b897-b6df0152413a",
   "metadata": {},
   "source": [
    "### Dataset caching tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0857db0-e18f-47af-a61f-28f09d0c6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def preload_datasets(paths: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Loads CSV files into a global cache if not already loaded.\n",
    "\n",
    "    This function helps to efficiently manage datasets by loading them once\n",
    "    and storing them in memory for future use. Without caching, you would\n",
    "    waste tokens describing dataset contents repeatedly in agent responses.\n",
    "\n",
    "    Args:\n",
    "        paths: A list of file paths to CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A message summarizing which datasets were loaded or already cached.\n",
    "    \"\"\"\n",
    "    loaded = []\n",
    "    cached = []\n",
    "    for path in paths:\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "            loaded.append(path)\n",
    "        else:\n",
    "            cached.append(path)\n",
    "\n",
    "    return (\n",
    "        f\"Loaded datasets: {loaded}\\n\"\n",
    "        f\"Already cached: {cached}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496c87e-2fdb-4b5b-9f91-8e0ddb020aa9",
   "metadata": {},
   "source": [
    "### Summarization tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481f591-96dd-4b7b-81f0-dc7567275eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional,Dict,Any\n",
    "\n",
    "@tool\n",
    "def get_dataset_summaries(dataset_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze multiple CSV files and return metadata summaries for each.\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (List[str]):\n",
    "            A list of file paths to CSV datasets.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]:\n",
    "            A list of summaries, one per dataset, each containing:\n",
    "            - \"file_name\": The path of the dataset file.\n",
    "            - \"column_names\": A list of column names in the dataset.\n",
    "            - \"data_types\": A dictionary mapping column names to their data types (as strings).\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for path in dataset_paths:\n",
    "        # Load and cache the dataset if not already cached\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "\n",
    "        df = DATAFRAME_CACHE[path]\n",
    "\n",
    "        # Build summary\n",
    "        summary = {\n",
    "            \"file_name\": path,\n",
    "            \"column_names\": df.columns.tolist(),\n",
    "            \"data_types\": df.dtypes.astype(str).to_dict()\n",
    "        }\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d7f1d-8216-4c38-a816-404799b9d29f",
   "metadata": {},
   "source": [
    "### DataFrame method execution tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2a1d4-0d50-47d4-add1-80f8bc236b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def call_dataframe_method(file_name: str, method: str) -> str:\n",
    "   \"\"\"\n",
    "   Execute a method on a DataFrame and return the result.\n",
    "   This tool lets you run simple DataFrame methods like 'head', 'tail', or 'describe'\n",
    "   on a dataset that has already been loaded and cached using 'preload_datasets'.\n",
    "   Args:\n",
    "       file_name (str): The path or name of the dataset in the global cache.\n",
    "       method (str): The name of the method to call on the DataFrame. Only no-argument\n",
    "                     methods are supported (e.g., 'head', 'describe', 'info').\n",
    "   Returns:\n",
    "       str: The output of the method as a formatted string, or an error message if\n",
    "            the dataset is not found or the method is invalid.\n",
    "   Example:\n",
    "       call_dataframe_method(file_name=\"data.csv\", method=\"head\")\n",
    "   \"\"\"\n",
    "   # Try to get the DataFrame from cache, or load it if not already cached\n",
    "   if file_name not in DATAFRAME_CACHE:\n",
    "       try:\n",
    "           DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "       except FileNotFoundError:\n",
    "           return f\"DataFrame '{file_name}' not found in cache or on disk.\"\n",
    "       except Exception as e:\n",
    "           return f\"Error loading '{file_name}': {str(e)}\"\n",
    "\n",
    "   df = DATAFRAME_CACHE[file_name]\n",
    "   func = getattr(df, method, None)\n",
    "   if not callable(func):\n",
    "       return f\"'{method}' is not a valid method of DataFrame.\"\n",
    "   try:\n",
    "       result = func()\n",
    "       return str(result)\n",
    "   except Exception as e:\n",
    "       return f\"Error calling '{method}' on '{file_name}': {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c09777-c8d7-47fc-a1cf-8eff91dafa41",
   "metadata": {},
   "source": [
    "### Model evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0a431-c475-4c10-8020-64523365f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assumes this global cache is shared\n",
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def evaluate_classification_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a classifier on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the classification target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with the model's accuracy score.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "\n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "@tool\n",
    "def evaluate_regression_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the regression target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with R² score and Mean Squared Error.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "\n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return {\n",
    "        \"r2_score\": r2,\n",
    "        \"mean_squared_error\": mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9674cb-011a-41bc-a8f6-12147299a829",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40fe05-da85-406f-8c9b-5e6a5560f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool, create_openai_tools_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 🧠 Step 2: Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a data science assistant. Use the available tools to analyze CSV files. \"\n",
    "     \"Your job is to determine whether each dataset is for classification or regression, based on its structure.\"),\n",
    "\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")  # Required for tool-calling agents\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845270f0-b195-404b-a610-c052634f1ec0",
   "metadata": {},
   "source": [
    "Now, create a chatbot object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602a3db-911e-443f-a956-e4d6a7e45870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", streaming=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70803d20-9d50-4b0b-8c4c-87df2c1e27ca",
   "metadata": {},
   "source": [
    "Create a list tool that contains all the tool objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad33d8-b3b8-49da-bd1f-0e13bdbd23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[list_csv_files, preload_datasets, get_dataset_summaries, call_dataframe_method, evaluate_classification_dataset, evaluate_regression_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a56a4-4cea-415c-8468-9b7dc603839d",
   "metadata": {},
   "source": [
    "### Agent creation and limitations\n",
    "\n",
    "Here, you'll create your agent using `create_openai_tools_agent()`, which combines the language model, tools, and prompt template into a functional agent. However, this raw agent has significant limitations when used directly. It only performs a single step of reasoning and tool usage per invocation, then returns its intermediate thought process rather than a final answer. This behavior occurs because the agent doesn't automatically manage the full loop of thinking, acting, observing results, and continuing to reason until reaching a complete solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c83dd-cd11-471c-83a3-3fac796ffd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d83b7f-0ea8-4f13-867c-b1d02e8b4185",
   "metadata": {},
   "source": [
    "### Agent executor ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f15f9-2bfb-4e39-bb09-685ec318e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)\n",
    "agent_executor.agent.stream_runnable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc2b9d-1bf6-4c15-aed3-736d75ef3c4f",
   "metadata": {},
   "source": [
    "You can now build a bot DataWizard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6172368-5a48-4139-b785-6f3c682f6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Ask questions about your dataset (type 'exit' to quit):\")\n",
    "\n",
    "while True:\n",
    "    user_input=input(\" You:\")\n",
    "    if user_input.strip().lower() in ['exit','quit']:\n",
    "        print(\"see ya later\")\n",
    "        break\n",
    "\n",
    "    result=agent_executor.invoke({\"input\":user_input})\n",
    "    print(f\"my Agent: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc106c5a-dfd0-4ddf-b025-3a2164a97a52",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "7ce133a1a2969efb00d5da0eb4e9eff7891a1c925a212bc3db02e66aeb950729"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
